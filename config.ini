[llm]
model_name : Llama-2-13b-chat
# meta-llama/Llama-2-70b-chat-hf Mixtral-8x7B-Instruct-v0.1
quantization : Q5_K_M
pipeline : llama_cpp
device_map : auto
task : text-generation
max_new_tokens : 1024
temperature : 0.1
n_batch : 1024
n_ctx : 6000
n_gpu_layers : -1
# The number of layers to put on the GPU. Mixtral-18, gemma-20
std_out : True
base_dir : ${root:root_path}/models

[chroma_client]
host : localhost
port : 8000
collection_name : arxiv
# embedding_type : sentence-transformers
# embedding_model : "all-mpnet-base-v2"
embedding_type : instructor-embedding
embedding_model : hkunlp/instructor-xl
;store_path : ${data:data_path}/vectordb
;allow_reset : True

[deeplake_client]
collection_name : arxiv
# embedding_type : sentence-transformers
# embedding_model : "all-mpnet-base-v2"
embedding_type : instructor-embedding
embedding_model : hkunlp/instructor-xl
store_path : ${data:data_path}/vectordb

[text_splitter]
chunk_size : 5000
chunk_overlap : 400

[multivec_retriever]
# store_path: data/docs
store_path : ${data:data_path}/doc_store
# namespace: UUID(8c9040b0-b5cd-4d7c-bc2e-737da1b24ebf)
namespace : 8c9040b0b5cd4d7cbc2e737da1b24ebf
id_key : doc_id
top_k : 3

[parse_pdf]
single_text_out : True
strategy : hi_res
infer_table_structure : True
extract_images : True
image_output_dir : None
add_captions_to_text : True
add_captions_to_blocks : True
table_as_html : True

[data]
data_path : ${root:root_path}/data

[env]
env_path : ${root:root_path}/.env

[root]
root_path : /home/ubuntu/volume_2k/Capstone_5

[quantize]
llama_cpp_path : ${root:root_path}
